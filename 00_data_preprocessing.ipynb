{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb7aaa4c-9e1b-4ee8-b044-b44232042347",
   "metadata": {},
   "source": [
    "# (00) Data Preprocessing\n",
    "\n",
    "This notebook prepares the entire ProphetLSTM-GAN project dataset.  \n",
    "We begin by configuring paths, verifying the repository structure, and ensuring that the raw dataset is correctly placed inside the `/data/raw/train/` and `/data/raw/test/` directories.\n",
    "\n",
    "### Objectives of this notebook:\n",
    "- Set up project directory paths\n",
    "- Scan raw training and testing CSV files\n",
    "- Validate required feature columns\n",
    "- Load and concatenate all valid training CSVs\n",
    "- Fit normalization scaler\n",
    "- Build sliding-window time-series tensors\n",
    "- Save processed numpy arrays for training, validation, and later test evaluation\n",
    "\n",
    "This step is critical because every later notebook (01_training and 02_evaluation) will rely on the processed files generated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd6d2d3-0305-4378-a4fd-fb864195ac4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Device:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3de08dbd-4e1c-4ffa-b350-dfdf216a5d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d080c1ee-8653-4bf0-9183-69aa35fc9b0d",
   "metadata": {},
   "source": [
    "## Dataset Setup — Spacecraft Thruster Firing Tests (Kaggle)\n",
    "\n",
    "This project uses the **Spacecraft Thruster Firing Tests Dataset**.\n",
    "\n",
    "**Kaggle Link:** https://www.kaggle.com/datasets/patrickfleith/spacecraft-thruster-firing-tests-dataset\n",
    "\n",
    "---\n",
    "\n",
    "This dataset contains **2,612 CSV files**, each representing a thruster firing sequence with signals such as:\n",
    "\n",
    "- `ton`  \n",
    "- `thrust`  \n",
    "- `mfr`  \n",
    "- `vl`  \n",
    "- `anomaly_code` (anomaly indicator)\n",
    "\n",
    "It is large (**~3 GB**), structured, and perfect for unsupervised anomaly detection using **LSTM-GANs**.\n",
    "\n",
    "To download the dataset, open the Kaggle link and hit the download button; you'll receive a zip file named something like `spacecraft-thruster-firing-tests-dataset.zip`. After unzipping it, you'll find two folders inside — `train` and `test` — each filled with hundreds of CSV files. Once extracted, your directory will look like `/path/to/downloaded/data/train/` and `/path/to/downloaded/data/test/`. Now just move these two folders into your project’s repository so the structure becomes `ProphetLSTM-GAN/data/raw/train/` and `ProphetLSTM-GAN/data/raw/test/`, where the `train` folder contains around 1267 CSVs and the `test` folder contains about 1344 CSVs. The `processed/` folder inside `data/` doesn’t need to exist yet — it’ll be automatically created later by your preprocessing script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1239562a-61a3-4e5e-b396-a1e16f9209d8",
   "metadata": {},
   "source": [
    "### Step 1: Project Directory Setup\n",
    "\n",
    "The following code:\n",
    "- Detects the project root (the folder where this notebook lives)\n",
    "- Creates references to all important directories:\n",
    "  - `data/raw/train/`\n",
    "  - `data/raw/test/`\n",
    "  - `data/processed/`\n",
    "- Ensures the processed directory exists\n",
    "- Prints the paths so we can visually confirm the structure before proceeding\n",
    "\n",
    "This ensures the notebook runs correctly no matter where the user executes it, as long as the repo structure is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1abe8596-1c8b-4b89-8be2-dd6f68130454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root      : C:\\Users\\aymisxx\\Documents\\GitHub\\ProphetLSTM-GAN\n",
      "Raw train dir     : C:\\Users\\aymisxx\\Documents\\GitHub\\ProphetLSTM-GAN\\data\\raw\\train\n",
      "Raw test dir      : C:\\Users\\aymisxx\\Documents\\GitHub\\ProphetLSTM-GAN\\data\\raw\\test\n",
      "Processed dir     : C:\\Users\\aymisxx\\Documents\\GitHub\\ProphetLSTM-GAN\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Detect the root of the repository (the directory where the notebook is opened)\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# Data directories\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "TRAIN_RAW_DIR = RAW_DIR / \"train\"\n",
    "TEST_RAW_DIR = RAW_DIR / \"test\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "# Create processed directory if missing\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Project root      :\", PROJECT_ROOT)\n",
    "print(\"Raw train dir     :\", TRAIN_RAW_DIR)\n",
    "print(\"Raw test dir      :\", TEST_RAW_DIR)\n",
    "print(\"Processed dir     :\", PROCESSED_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52fcef9-f879-40dc-8811-06b5dccdf3f9",
   "metadata": {},
   "source": [
    "## Step 2: Scan Raw Training and Test CSV Files\n",
    "\n",
    "Before we touch any preprocessing logic, we first verify that the raw dataset is correctly placed in:\n",
    "\n",
    "- `data/raw/train/`  → training CSV files  \n",
    "- `data/raw/test/`   → testing CSV files  \n",
    "\n",
    "In this step, we will:\n",
    "\n",
    "- Recursively search for all `.csv` files inside the train and test folders  \n",
    "- Count how many files are found in each split  \n",
    "- Print a short preview of the first few filenames  \n",
    "\n",
    "This acts as a sanity check to confirm:\n",
    "- The dataset is actually present\n",
    "- The train/test separation is in the expected directories\n",
    "- There are no obvious path mistakes before we proceed to loading and validating columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "914d493e-fe07-48cb-8541-42661d5ada3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1267 train CSV files.\n",
      "Found 1344 test CSV files.\n",
      "\n",
      "Sample train files:\n",
      "  - 00001_001_SN01_24bars_ssf.csv\n",
      "  - 00002_002_SN01_21bars_ssf.csv\n",
      "  - 00003_003_SN01_18bars_ssf.csv\n",
      "  - 00004_004_SN01_15bars_ssf.csv\n",
      "  - 00005_005_SN01_12bars_ssf.csv\n",
      "\n",
      "Sample test files:\n",
      "  - 01269_001_SN13_24bars_ssf.csv\n",
      "  - 01270_002_SN13_21bars_ssf.csv\n",
      "  - 01271_003_SN13_18bars_ssf.csv\n",
      "  - 01272_004_SN13_15bars_ssf.csv\n",
      "  - 01273_005_SN13_12bars_ssf.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Collect all CSV files in train and test directories\n",
    "train_csv_files = sorted(TRAIN_RAW_DIR.glob(\"*.csv\"))\n",
    "test_csv_files = sorted(TEST_RAW_DIR.glob(\"*.csv\"))\n",
    "\n",
    "print(f\"Found {len(train_csv_files)} train CSV files.\")\n",
    "print(f\"Found {len(test_csv_files)} test CSV files.\\n\")\n",
    "\n",
    "# Preview a few train files\n",
    "if train_csv_files:\n",
    "    print(\"Sample train files:\")\n",
    "    for p in train_csv_files[:5]:\n",
    "        print(\"  -\", p.name)\n",
    "else:\n",
    "    print(\"WARNING: No train CSV files found in\", TRAIN_RAW_DIR)\n",
    "\n",
    "print()\n",
    "\n",
    "# Preview a few test files\n",
    "if test_csv_files:\n",
    "    print(\"Sample test files:\")\n",
    "    for p in test_csv_files[:5]:\n",
    "        print(\"  -\", p.name)\n",
    "else:\n",
    "    print(\"WARNING: No test CSV files found in\", TEST_RAW_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f2be4-cfe7-473f-baf2-30e4465585a7",
   "metadata": {},
   "source": [
    "## Step 3: Inspect a Sample Training File & Define Feature / Label Columns\n",
    "\n",
    "Before we build sliding windows or normalize anything, we need to clearly understand **what signals** each CSV file contains and decide:\n",
    "\n",
    "- Which columns are **input features** to the model\n",
    "- Which column is the **label** (anomaly indicator)\n",
    "\n",
    "From the dataset, each CSV file includes:\n",
    "\n",
    "- `time`\n",
    "- `ton`\n",
    "- `thrust`\n",
    "- `mfr`\n",
    "- `vl`\n",
    "- `anomaly_code`\n",
    "\n",
    "For this project, we will use:\n",
    "\n",
    "- **Input feature columns (model inputs)**:\n",
    "  - `time`\n",
    "  - `ton`\n",
    "  - `thrust`\n",
    "  - `mfr`\n",
    "  - `vl`\n",
    "\n",
    "- **Label column (target)**:\n",
    "  - `anomaly_code`\n",
    "\n",
    "In this step, we will:\n",
    "\n",
    "1. Load a single sample training CSV file.\n",
    "2. Print:\n",
    "   - Its shape (rows × columns)\n",
    "   - The list of column names\n",
    "3. Define:\n",
    "   - A list of feature columns: `FEATURE_COLS`\n",
    "   - A label column: `LABEL_COL`\n",
    "4. Verify that all chosen feature columns are present in the sample file.\n",
    "\n",
    "This ensures that we lock in a consistent feature set and can later enforce this across all training and test files.\n",
    "\n",
    "The next code cell will:\n",
    "\n",
    "1. Import `pandas` as `pd` for CSV loading.\n",
    "2. Select the **first** training CSV file from `train_csv_files`\n",
    "   (which we already collected earlier from `data/raw/train/`).\n",
    "3. Load that file fully with `pd.read_csv`.\n",
    "4. Print:\n",
    "   - The file name\n",
    "   - The DataFrame shape\n",
    "   - The list of available columns\n",
    "5. Define:\n",
    "   - `FEATURE_COLS = [\"time\", \"ton\", \"thrust\", \"mfr\", \"vl\"]`\n",
    "   - `LABEL_COL = \"anomaly_code\"`\n",
    "6. Check:\n",
    "   - Whether each column in `FEATURE_COLS` exists in the DataFrame.\n",
    "   - If any are missing, print a warning; otherwise, confirm success.\n",
    "\n",
    "This step **does not** modify or save anything yet — it only inspects the schema and locks in the feature/label definitions we will use throughout preprocessing, training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7c7c771-85fa-4102-80db-4d31e3f79865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training file: 00001_001_SN01_24bars_ssf.csv\n",
      "\n",
      "Data shape (rows, columns): (30600, 6)\n",
      "\n",
      "Available columns:\n",
      "  - time\n",
      "  - ton\n",
      "  - thrust\n",
      "  - mfr\n",
      "  - vl\n",
      "  - anomaly_code\n",
      "\n",
      "Feature columns (model inputs):\n",
      "  - time\n",
      "  - ton\n",
      "  - thrust\n",
      "  - mfr\n",
      "  - vl\n",
      "\n",
      "Label column (target):\n",
      "  - anomaly_code\n",
      "\n",
      "All feature columns are present in the sample file.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Safety check: make sure we actually have train files\n",
    "if not train_csv_files:\n",
    "    raise RuntimeError(f\"No training CSV files found in {TRAIN_RAW_DIR}\")\n",
    "\n",
    "# Pick one sample train file to inspect (first in sorted order)\n",
    "sample_train_path = train_csv_files[0]\n",
    "print(\"Sample training file:\", sample_train_path.name)\n",
    "\n",
    "# Load the CSV\n",
    "df_sample = pd.read_csv(sample_train_path)\n",
    "\n",
    "print(\"\\nData shape (rows, columns):\", df_sample.shape)\n",
    "print(\"\\nAvailable columns:\")\n",
    "for col in df_sample.columns:\n",
    "    print(\"  -\", col)\n",
    "\n",
    "\n",
    "# Define feature columns (model inputs) and label column (target)\n",
    "\n",
    "FEATURE_COLS = [\"time\", \"ton\", \"thrust\", \"mfr\", \"vl\"]\n",
    "LABEL_COL = \"anomaly_code\"\n",
    "\n",
    "print(\"\\nFeature columns (model inputs):\")\n",
    "for c in FEATURE_COLS:\n",
    "    print(\"  -\", c)\n",
    "\n",
    "print(\"\\nLabel column (target):\")\n",
    "print(\"  -\", LABEL_COL)\n",
    "\n",
    "# Check presence of all feature columns in this sample file\n",
    "missing_features = [c for c in FEATURE_COLS if c not in df_sample.columns]\n",
    "\n",
    "if missing_features:\n",
    "    print(\"\\nWARNING: The following feature columns are MISSING in the sample file:\")\n",
    "    for c in missing_features:\n",
    "        print(\"  -\", c)\n",
    "else:\n",
    "    print(\"\\nAll feature columns are present in the sample file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3015ddd-feeb-4fc9-b741-82b2737153a0",
   "metadata": {},
   "source": [
    "## Step 4: Validate All Training Files Against the Feature Column Schema\n",
    "\n",
    "Now that we have identified the correct set of feature columns:\n",
    "\n",
    "- `time`\n",
    "- `ton`\n",
    "- `thrust`\n",
    "- `mfr`\n",
    "- `vl`\n",
    "\n",
    "we need to check all **1267 training CSV files** and verify that each one contains all\n",
    "required input signals.\n",
    "\n",
    "This step will:\n",
    "\n",
    "- Loop through every CSV inside `data/raw/train/`\n",
    "- Attempt to read the header of each file\n",
    "- Check whether all `FEATURE_COLS` are present\n",
    "- Build two lists:\n",
    "  - `valid_train_files`   → files that contain all required features  \n",
    "  - `skipped_train_files` → files missing any required feature, or unreadable\n",
    "\n",
    "At the end, we will print:\n",
    "- Total number of train files\n",
    "- Number of valid files\n",
    "- Number of skipped files\n",
    "- Examples of skipped files\n",
    "\n",
    "This ensures that only consistent, clean files enter the preprocessing pipeline, preventing silent failures later during scaling or sliding-window generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e6aa5f7-ece8-459b-ada3-04074d8f1200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train CSV files      : 1267\n",
      "Valid train files (usable) : 1267\n",
      "Skipped train files        : 0\n",
      "\n",
      "No train files were skipped.\n"
     ]
    }
   ],
   "source": [
    "valid_train_files = []\n",
    "skipped_train_files = []\n",
    "\n",
    "for path in train_csv_files:\n",
    "    try:\n",
    "        # Read just the first few rows to get column names\n",
    "        df_head = pd.read_csv(path, nrows=5)\n",
    "    except Exception as e:\n",
    "        skipped_train_files.append({\n",
    "            \"file\": path.name,\n",
    "            \"reason\": f\"read_error: {e}\"\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # Check required feature presence\n",
    "    missing = [c for c in FEATURE_COLS if c not in df_head.columns]\n",
    "    \n",
    "    if missing:\n",
    "        skipped_train_files.append({\n",
    "            \"file\": path.name,\n",
    "            \"reason\": f\"missing_columns: {missing}\"\n",
    "        })\n",
    "    else:\n",
    "        valid_train_files.append(path)\n",
    "\n",
    "print(f\"Total train CSV files      : {len(train_csv_files)}\")\n",
    "print(f\"Valid train files (usable) : {len(valid_train_files)}\")\n",
    "print(f\"Skipped train files        : {len(skipped_train_files)}\\n\")\n",
    "\n",
    "# Show first few skipped examples\n",
    "if skipped_train_files:\n",
    "    print(\"Examples of skipped train files:\")\n",
    "    for entry in skipped_train_files[:10]:\n",
    "        print(f\"  - {entry['file']}  →  {entry['reason']}\")\n",
    "else:\n",
    "    print(\"No train files were skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c59dc77-5c24-4509-9a40-7a784ca5be21",
   "metadata": {},
   "source": [
    "## Step 5: Load and Concatenate All Valid Training CSV Files\n",
    "\n",
    "Now that we have verified that **all 1267 training files** contain the required feature\n",
    "columns, we can safely load them into memory.\n",
    "\n",
    "In this step, we will:\n",
    "\n",
    "1. Loop through each file in `valid_train_files`\n",
    "2. Load the full CSV using `pandas.read_csv`\n",
    "3. Extract:\n",
    "   - Only the `FEATURE_COLS` (time, ton, thrust, mfr, vl)\n",
    "   - And store the corresponding label column (`anomaly_code`) separately\n",
    "4. Accumulate:\n",
    "   - `all_features_raw` — list of numpy arrays containing feature data from each file  \n",
    "   - `all_labels_raw`   — list of numpy arrays containing label data from each file\n",
    "\n",
    "After processing every file, we will:\n",
    "\n",
    "- Concatenate all feature arrays into `train_features_raw`  \n",
    "- Concatenate all label arrays into `train_labels_raw`  \n",
    "- Print the final shapes for verification\n",
    "\n",
    "At this stage:\n",
    "- No scaling is applied yet  \n",
    "- No sliding windows are created  \n",
    "- This step only assembles the raw, unified dataset into clean numpy arrays\n",
    "\n",
    "This will form the foundation for normalization and windowing in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae4cd9b6-1bdd-4eb1-ba28-62144c16bf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using feature columns: ['ton', 'thrust', 'mfr', 'vl'] \n",
      "\n",
      "Reloading training CSV files using corrected feature and label handling...\n",
      "\n",
      "Finished loading all train files with corrected features and labels.\n",
      "\n",
      "Total training samples (rows): 81180322\n",
      "Feature matrix shape        : (81180322, 4)\n",
      "Label array shape           : (81180322,)\n",
      "Unique labels               : [0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Updated feature & label config\n",
    "FEATURE_COLS = [\"ton\", \"thrust\", \"mfr\", \"vl\"]   # numeric only\n",
    "LABEL_COL = \"anomaly_code\"                      # raw label → we'll binarize\n",
    "\n",
    "print(\"Using feature columns:\", FEATURE_COLS, \"\\n\")\n",
    "\n",
    "all_features_raw = []\n",
    "all_labels_raw = []\n",
    "\n",
    "# Optional: suppress annoying runtime warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Reloading training CSV files using corrected feature and label handling...\\n\")\n",
    "\n",
    "for i, path in enumerate(valid_train_files):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # FEATURES (always numeric)\n",
    "    feat = df[FEATURE_COLS].astype(float).values\n",
    "\n",
    "    # LABELS (binarize anomaly_code)\n",
    "    raw_labels = df[LABEL_COL]\n",
    "\n",
    "    # Binary mapping:\n",
    "    #   NaN / 0 / \"0\" / empty → 0 (normal)\n",
    "    #   anything else        → 1 (anomaly)\n",
    "    is_normal = (\n",
    "        raw_labels.isna()\n",
    "        | (raw_labels == 0)\n",
    "        | (raw_labels == \"0\")\n",
    "        | (raw_labels == \"\")\n",
    "        | (raw_labels == \" \")\n",
    "    )\n",
    "    clean_labels = np.where(is_normal, 0, 1).astype(int)\n",
    "\n",
    "    all_features_raw.append(feat)\n",
    "    all_labels_raw.append(clean_labels)\n",
    "\n",
    "# CONCATENATE EVERYTHING\n",
    "train_features_raw = np.concatenate(all_features_raw, axis=0)\n",
    "train_labels_raw   = np.concatenate(all_labels_raw, axis=0)\n",
    "\n",
    "print(\"Finished loading all train files with corrected features and labels.\\n\")\n",
    "print(\"Total training samples (rows):\", train_features_raw.shape[0])\n",
    "print(\"Feature matrix shape        :\", train_features_raw.shape)\n",
    "print(\"Label array shape           :\", train_labels_raw.shape)\n",
    "print(\"Unique labels               :\", np.unique(train_labels_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26fb64e-141b-4ba7-950c-faeef35eb3b4",
   "metadata": {},
   "source": [
    "## Step 6: Fit StandardScaler on Normal Data and Normalize Features\n",
    "\n",
    "To stabilize LSTM-GAN training, all input features must be normalized.\n",
    "\n",
    "We will now:\n",
    "\n",
    "1. Use the unified raw arrays from the previous step:\n",
    "   - `train_features_raw` — shape `(N, 4)`\n",
    "   - `train_labels_raw`   — shape `(N,)`, values in `{0, 1}`\n",
    "\n",
    "2. Select only **normal samples** (label = 0) to fit the scaler.  \n",
    "   This is standard in anomaly detection so that the normalization statistics\n",
    "   reflect healthy operation only.\n",
    "\n",
    "3. Fit a `StandardScaler` (zero mean, unit variance) on these normal samples.\n",
    "\n",
    "4. Transform **all** training samples (normal + anomalous) using this scaler,\n",
    "   producing `train_features_scaled`.\n",
    "\n",
    "5. Save the fitted scaler and feature column names as:\n",
    "   - `data/processed/scaler.pkl`\n",
    "\n",
    "Later notebooks (`01_training.ipynb` and `02_evaluation.ipynb`) will load this same scaler to keep normalization consistent across train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb9cec99-7ad4-483b-a870-cbc799caedc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw feature matrix shape : (81180322, 4)\n",
      "Raw label array shape    : (81180322,)\n",
      "\n",
      "Fitting StandardScaler on normal samples only...\n",
      "Normal sample count      : 80687802\n",
      "\n",
      "Scaling complete.\n",
      "Scaled feature matrix shape: (81180322, 4)\n",
      "\n",
      "Saved scaler to: C:\\Users\\aymisxx\\Documents\\GitHub\\ProphetLSTM-GAN\\data\\processed\\scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Sanity check\n",
    "print(\"Raw feature matrix shape :\", train_features_raw.shape)\n",
    "print(\"Raw label array shape    :\", train_labels_raw.shape)\n",
    "\n",
    "# 1) Select only normal samples (label = 0)\n",
    "normal_mask = (train_labels_raw == 0)\n",
    "X_normal = train_features_raw[normal_mask]\n",
    "\n",
    "print(\"\\nFitting StandardScaler on normal samples only...\")\n",
    "print(\"Normal sample count      :\", X_normal.shape[0])\n",
    "\n",
    "# 2) Fit scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_normal)\n",
    "\n",
    "# 3) Transform all training features\n",
    "train_features_scaled = scaler.transform(train_features_raw)\n",
    "\n",
    "print(\"\\nScaling complete.\")\n",
    "print(\"Scaled feature matrix shape:\", train_features_scaled.shape)\n",
    "\n",
    "# 4) Save scaler + metadata\n",
    "scaler_path = PROCESSED_DIR / \"scaler.pkl\"\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"scaler\": scaler,\n",
    "        \"feature_cols\": FEATURE_COLS,\n",
    "    },\n",
    "    scaler_path,\n",
    ")\n",
    "\n",
    "print(\"\\nSaved scaler to:\", scaler_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527bec38-c23c-40d1-9aa3-23769c2343b8",
   "metadata": {},
   "source": [
    "## Step 7: Build Sliding Windows, Split into Train / Validation, and Save\n",
    "\n",
    "Right now we have:\n",
    "\n",
    "- `train_features_scaled` with shape `(N_timesteps, 4)`  \n",
    "- `valid_train_files` listing all 1267 train CSVs  \n",
    "- A fitted `StandardScaler` and saved `scaler.pkl`\n",
    "\n",
    "Sequence models (LSTMs, GANs) don’t work directly on arbitrarily long time series.\n",
    "Instead, they expect **fixed-length windows** like:\n",
    "\n",
    "- shape `(window_size, n_features)` per sample  \n",
    "- e.g. `(128, 4)` for this project\n",
    "\n",
    "In this step, we will:\n",
    "\n",
    "1. Define sliding-window hyperparameters:\n",
    "   - `WINDOW_SIZE = 128` timesteps\n",
    "   - `WINDOW_STRIDE = 5` between window starts\n",
    "   - `MAX_WINDOWS_PER_FILE = 300` to cap memory\n",
    "\n",
    "2. Implement a helper `make_sliding_windows(sequence_array, ...)` that:\n",
    "   - Takes one scaled sequence for a single CSV: shape `(T, 4)`\n",
    "   - Returns windows of shape `(num_windows, 128, 4)`\n",
    "   - Uses stride and max cap to keep things tractable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f4fed8f-3bd8-4b2f-99db-bedc3d9f1f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows: (380100, 128, 4)\n",
      "X_train_final: (304080, 128, 4)\n",
      "X_val: (76020, 128, 4)\n",
      "Saved to: C:\\Users\\aymisxx\\Documents\\GitHub\\ProphetLSTM-GAN\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Config\n",
    "WINDOW_SIZE = 128\n",
    "WINDOW_STRIDE = 5\n",
    "MAX_WINDOWS_PER_FILE = 300\n",
    "FEATURE_COLS = [\"ton\", \"thrust\", \"mfr\", \"vl\"]\n",
    "\n",
    "def make_sliding_windows(seq, wsize=WINDOW_SIZE, stride=WINDOW_STRIDE, max_w=MAX_WINDOWS_PER_FILE):\n",
    "    T, F = seq.shape\n",
    "    if T < wsize:\n",
    "        return np.empty((0, wsize, F), dtype=np.float32)\n",
    "    starts = np.arange(0, T - wsize + 1, stride)\n",
    "    if len(starts) > max_w:\n",
    "        starts = starts[:max_w]\n",
    "    return np.stack([seq[s:s+wsize] for s in starts], axis=0).astype(np.float32)\n",
    "\n",
    "X_train_windows = []\n",
    "\n",
    "for path in valid_train_files:\n",
    "    df = pd.read_csv(path)\n",
    "    feats = df[FEATURE_COLS].astype(float).values\n",
    "    scaled = scaler.transform(feats)\n",
    "    w = make_sliding_windows(scaled)\n",
    "    if w.shape[0] > 0:\n",
    "        X_train_windows.append(w)\n",
    "\n",
    "# Concatenate all windows\n",
    "X_all = np.concatenate(X_train_windows, axis=0)\n",
    "\n",
    "# Split 80/20\n",
    "split = int(0.8 * X_all.shape[0])\n",
    "X_train_final = X_all[:split]\n",
    "X_val = X_all[split:]\n",
    "\n",
    "# Save\n",
    "np.save(PROCESSED_DIR / \"X_train_final.npy\", X_train_final)\n",
    "np.save(PROCESSED_DIR / \"X_val.npy\", X_val)\n",
    "\n",
    "print(\"Total windows:\", X_all.shape)\n",
    "print(\"X_train_final:\", X_train_final.shape)\n",
    "print(\"X_val:\", X_val.shape)\n",
    "print(\"Saved to:\", PROCESSED_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e87911e4-161e-4bf2-8a1a-11f4238210d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test windows...\n",
      "X_test shape: (403200, 128, 4)\n",
      "Saved: C:\\Users\\aymisxx\\Documents\\GitHub\\ProphetLSTM-GAN\\data\\processed\\X_test.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Config (same as train)\n",
    "WINDOW_SIZE = 128\n",
    "WINDOW_STRIDE = 5\n",
    "MAX_WINDOWS_PER_FILE = 300\n",
    "FEATURE_COLS = [\"ton\", \"thrust\", \"mfr\", \"vl\"]\n",
    "\n",
    "def make_sliding_windows(seq, wsize=WINDOW_SIZE, stride=WINDOW_STRIDE, max_w=MAX_WINDOWS_PER_FILE):\n",
    "    T, F = seq.shape\n",
    "    if T < wsize:\n",
    "        return np.empty((0, wsize, F), dtype=np.float32)\n",
    "    starts = np.arange(0, T - wsize + 1, stride)\n",
    "    if len(starts) > max_w:\n",
    "        starts = starts[:max_w]\n",
    "    return np.stack([seq[s:s+wsize] for s in starts], axis=0).astype(np.float32)\n",
    "\n",
    "X_test_windows = []\n",
    "\n",
    "print(\"Generating test windows...\")\n",
    "\n",
    "for path in test_csv_files:\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Extract features\n",
    "    feats = df[FEATURE_COLS].astype(float).values\n",
    "\n",
    "    # Scale using the SAME scaler from training\n",
    "    scaled = scaler.transform(feats)\n",
    "\n",
    "    # Generate windows\n",
    "    w = make_sliding_windows(scaled)\n",
    "    if w.shape[0] > 0:\n",
    "        X_test_windows.append(w)\n",
    "\n",
    "# Concatenate all windows\n",
    "X_test = np.concatenate(X_test_windows, axis=0)\n",
    "\n",
    "# Save\n",
    "np.save(PROCESSED_DIR / \"X_test.npy\", X_test)\n",
    "\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Saved:\", PROCESSED_DIR / \"X_test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1def4b00-dac4-4744-baa2-dda1062ec7a5",
   "metadata": {},
   "source": [
    "### Notebook \"00_data_preprocessing.ipynb\" complete."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
